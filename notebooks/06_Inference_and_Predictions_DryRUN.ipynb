{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43cc2fcf-ea45-452a-8660-72ca41e8901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NOTEBOOK 06 ‚Äî INFERENCE & PREDICTIONS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üöÄ CIVICPULSE TRAINING CONFIGURATION (Benchmark-Optimized)\n",
      "======================================================================\n",
      "Device           : cpu\n",
      "VRAM Available   : 0.0 GB\n",
      "Batch Size       : 64\n",
      "Data Mode        : normal\n",
      "Patch Size       : 256√ó256 cells\n",
      "ConvLSTM         : 64 hidden, 2 layers\n",
      "Learning Rate    : 0.001\n",
      "Sequence Length  : 4 timesteps\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 ‚Äî Imports + Config\n",
    "import sys, os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import rasterio.transform\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm as tqdm_nb\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from src.config import TrainingConfig\n",
    "\n",
    "config    = TrainingConfig()\n",
    "device    = config.DEVICE\n",
    "LOAD_MODE = config.DATA_MODE\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NOTEBOOK 06 ‚Äî INFERENCE & PREDICTIONS\")\n",
    "print(\"=\" * 70)\n",
    "config.print_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b7b6a5-b172-4cb1-9dc8-b2d07d456393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded checkpoint  (R¬≤=-0.05233561992645264)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvLSTMEncoderDecoder(\n",
       "  (encoder_cells): ModuleList(\n",
       "    (0): ConvLSTMCell(\n",
       "      (conv_gates): Conv2d(65, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_candidate): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (1): ConvLSTMCell(\n",
       "      (conv_gates): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_candidate): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (decoder_cells): ModuleList(\n",
       "    (0-1): 2 x ConvLSTMCell(\n",
       "      (conv_gates): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv_candidate): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (output_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2 ‚Äî Load Model (config-aware)\n",
    "# [Paste ConvLSTMCell + ConvLSTMEncoderDecoder here ‚Äî same as NB04 Cells 2, 3]\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        padding = kernel_size // 2\n",
    "        self.conv_gates = nn.Conv2d(\n",
    "            in_channels + hidden_channels, 2 * hidden_channels, kernel_size, padding=padding)\n",
    "        self.conv_candidate = nn.Conv2d(\n",
    "            in_channels + hidden_channels, hidden_channels, kernel_size, padding=padding)\n",
    "\n",
    "    def forward(self, inputs, hidden_state):\n",
    "        h, c = hidden_state\n",
    "        combined = torch.cat([inputs, h], dim=1)\n",
    "        gates = self.conv_gates(combined)\n",
    "        reset_gate, update_gate = torch.split(gates, self.hidden_channels, dim=1)\n",
    "        reset_gate  = torch.sigmoid(reset_gate)\n",
    "        update_gate = torch.sigmoid(update_gate)\n",
    "        combined_candidate = torch.cat([inputs, reset_gate * h], dim=1)\n",
    "        candidate = torch.tanh(self.conv_candidate(combined_candidate))\n",
    "        new_c = (1 - update_gate) * c + update_gate * candidate\n",
    "        new_h = torch.tanh(new_c) * update_gate + (1 - update_gate) * h\n",
    "        return new_h, new_c\n",
    "\n",
    "class ConvLSTMEncoderDecoder(nn.Module):\n",
    "    def __init__(self, in_channels=1,\n",
    "                 hidden_channels=None, num_layers=None, kernel_size=3):\n",
    "        super().__init__()\n",
    "        # Pull from config if not explicitly passed\n",
    "        _cfg = TrainingConfig()\n",
    "        hidden_channels = hidden_channels or _cfg.HIDDEN_CHANNELS\n",
    "        num_layers      = num_layers      or _cfg.NUM_LAYERS\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers      = num_layers\n",
    "        self.encoder_cells = nn.ModuleList([\n",
    "            ConvLSTMCell(in_channels if i == 0 else hidden_channels,\n",
    "                         hidden_channels, kernel_size)\n",
    "            for i in range(num_layers)])\n",
    "        self.decoder_cells = nn.ModuleList([\n",
    "            ConvLSTMCell(hidden_channels, hidden_channels, kernel_size)\n",
    "            for _ in range(num_layers)])\n",
    "        self.output_conv = nn.Conv2d(hidden_channels, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        h = [torch.zeros(B, self.hidden_channels, H, W, device=x.device, dtype=x.dtype)\n",
    "             for _ in range(self.num_layers)]\n",
    "        c = [torch.zeros(B, self.hidden_channels, H, W, device=x.device, dtype=x.dtype)\n",
    "             for _ in range(self.num_layers)]\n",
    "        for t in range(T):\n",
    "            xt = x[:, t]\n",
    "            for layer in range(self.num_layers):\n",
    "                h[layer], c[layer] = self.encoder_cells[layer](\n",
    "                    xt if layer == 0 else h[layer-1], (h[layer], c[layer]))\n",
    "        for layer in range(self.num_layers):\n",
    "            inp = h[layer-1] if layer > 0 else h[0]\n",
    "            h[layer], c[layer] = self.decoder_cells[layer](inp, (h[layer], c[layer]))\n",
    "        return self.output_conv(h[-1])   # (B, 1, H, W)\n",
    "\n",
    "model_path = \"models/checkpoints/best_model.pt\"\n",
    "model = ConvLSTMEncoderDecoder().to(device)   # pulls HIDDEN_CHANNELS/NUM_LAYERS from config\n",
    "if Path(model_path).exists():\n",
    "    ckpt = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    print(f\"‚úÖ Loaded checkpoint  (R¬≤={ckpt.get('r2', 'N/A')})\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No checkpoint found ‚Äî using untrained weights (demo only)\")\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c67f7d99-9a9f-4fb9-b102-f45d73656029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Normal mode...\n",
      "  Data shape: (5, 1634, 997)  |  {'years': '2000,2005,2010,2015,2020'}\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 ‚Äî Load Data (mode-aware)\n",
    "h5_path = \"data/processed/india_sample.h5\"\n",
    "\n",
    "if LOAD_MODE == \"hdf5\":\n",
    "    print(\"üìÇ HDF5 mode...\")\n",
    "    with h5py.File(h5_path, \"r\") as h5:\n",
    "        data     = h5[\"population_data\"][:]\n",
    "        metadata = {k: h5.attrs[k] for k in h5.attrs}\n",
    "else:\n",
    "    print(\"üìÇ Normal mode...\")\n",
    "    tel  = np.load(\"data/processed/telangana_population_sequence.npy\")\n",
    "    maha = np.load(\"data/processed/maharashtra_population_sequence.npy\")\n",
    "    T, H1, W1 = tel.shape\n",
    "    _,  H2, W2 = maha.shape\n",
    "    maxH, maxW = max(H1,H2), max(W1,W2)\n",
    "    tel  = np.pad(tel,  ((0,0),(0,maxH-H1),(0,maxW-W1)))\n",
    "    maha = np.pad(maha, ((0,0),(0,maxH-H2),(0,maxW-W2)))\n",
    "    data     = np.concatenate([tel, maha], axis=1).astype(np.float32)\n",
    "    metadata = {\"years\": \"2000,2005,2010,2015,2020\"}\n",
    "\n",
    "print(f\"  Data shape: {data.shape}  |  {metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab41e040-b344-4a72-be22-7a6284fd7888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 ‚Äî Patch-based Prediction Function\n",
    "def predict_full(model, data_sequence, patch_size=512, overlap=64, device=\"cpu\"):\n",
    "    T, H, W    = data_sequence.shape\n",
    "    output     = np.zeros((H, W), dtype=np.float32)\n",
    "    count_map  = np.zeros((H, W), dtype=np.float32)\n",
    "    stride     = patch_size - overlap\n",
    "    patches    = [(y, x)\n",
    "                  for y in range(0, H - patch_size + 1, stride)\n",
    "                  for x in range(0, W - patch_size + 1, stride)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for y, x in tqdm_nb(patches, desc=\"Predicting patches\"):\n",
    "            patch = data_sequence[:, y:y+patch_size, x:x+patch_size]\n",
    "            X     = torch.from_numpy(patch[:4].copy()).float() \\\n",
    "                        .unsqueeze(0).unsqueeze(2).to(device)\n",
    "            pred  = model(X).squeeze().cpu().numpy()\n",
    "            output[y:y+patch_size, x:x+patch_size]    += pred\n",
    "            count_map[y:y+patch_size, x:x+patch_size] += 1\n",
    "\n",
    "    return output / (count_map + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88b7c28d-8206-41f2-8da5-99964eee5131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ Predicting 2025 (input: 2005‚Äì2020)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fea820aa43542dab0124371f70d0496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting patches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shape: (1634, 997)  Range: -2‚Äì18\n",
      "\n",
      "üîÆ Predicting 2030 (input: 2010‚Äì2025)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4495af3b26814b6e9ef8f744a32f8091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting patches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shape: (1634, 997)  Range: -2‚Äì18\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 ‚Äî Generate 2025 & 2030 Predictions\n",
    "predictions = {}\n",
    "\n",
    "print(\"üîÆ Predicting 2025 (input: 2005‚Äì2020)...\")\n",
    "pred_2025 = predict_full(model, data[1:], device=device)\n",
    "predictions[\"2025\"] = pred_2025\n",
    "print(f\"  Shape: {pred_2025.shape}  Range: {pred_2025.min():.0f}‚Äì{pred_2025.max():.0f}\")\n",
    "\n",
    "print(\"\\nüîÆ Predicting 2030 (input: 2010‚Äì2025)...\")\n",
    "data_2010_2025 = np.concatenate([data[2:], pred_2025[np.newaxis]], axis=0)\n",
    "pred_2030 = predict_full(model, data_2010_2025, device=device)\n",
    "predictions[\"2030\"] = pred_2030\n",
    "print(f\"  Shape: {pred_2030.shape}  Range: {pred_2030.min():.0f}‚Äì{pred_2030.max():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f16ca814-5613-4c4c-9da9-352b1b3ffa72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Saved data\\projections\\population_prediction_2025.tif\n",
      "  ‚úÖ Saved data\\projections\\population_prediction_2030.tif\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 ‚Äî Save as GeoTIFF\n",
    "proj_dir = Path(\"data/projections\")\n",
    "proj_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ref_tifs = list(Path(\"data/processed\").glob(\"india_pop_clipped_*.tif\"))\n",
    "if ref_tifs:\n",
    "    with rasterio.open(ref_tifs[0]) as src:\n",
    "        base_profile = src.profile\n",
    "else:\n",
    "    base_profile = dict(\n",
    "        driver=\"GTiff\", dtype=\"float32\", nodata=None,\n",
    "        width=pred_2025.shape[1], height=pred_2025.shape[0],\n",
    "        count=1, crs=\"EPSG:4326\",\n",
    "        transform=rasterio.transform.Affine(1, 0, 72, 0, -1, 35))\n",
    "\n",
    "for year, pred in predictions.items():\n",
    "    out_path = proj_dir / f\"population_prediction_{year}.tif\"\n",
    "    base_profile.update(dtype=\"float32\", width=pred.shape[1], height=pred.shape[0], count=1)\n",
    "    with rasterio.open(out_path, \"w\", **base_profile) as dst:\n",
    "        dst.write(pred, 1)\n",
    "    print(f\"  ‚úÖ Saved {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ba1fbeb-48cb-4412-9b59-1fd59b409640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80e8dc2eac144fea3019cbe1681c237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting patches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREDICTION VALIDATION (2020 actual vs predicted)\n",
      "======================================================================\n",
      "  R¬≤  : -0.029\n",
      "  MAE : 182.9 people/km¬≤\n",
      "  RMSE: 1062.4 people/km¬≤\n",
      "\n",
      "‚úÖ Next: Notebook 07 ‚Äî Gap Analysis\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 ‚Äî Validation vs Actual 2020\n",
    "actual_2020 = data[4]\n",
    "pred_2020   = predict_full(model, data[:4], device=device)\n",
    "\n",
    "mae  = np.abs(pred_2020 - actual_2020).mean()\n",
    "rmse = np.sqrt(((pred_2020 - actual_2020) ** 2).mean())\n",
    "ss_res = ((pred_2020 - actual_2020) ** 2).sum()\n",
    "ss_tot = ((actual_2020 - actual_2020.mean()) ** 2).sum()\n",
    "r2 = 1 - ss_res / ss_tot\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREDICTION VALIDATION (2020 actual vs predicted)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  R¬≤  : {r2:.3f}\")\n",
    "print(f\"  MAE : {mae:.1f} people/km¬≤\")\n",
    "print(f\"  RMSE: {rmse:.1f} people/km¬≤\")\n",
    "print(\"\\n‚úÖ Next: Notebook 07 ‚Äî Gap Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f70337-4a14-4ec1-ad49-21da791934b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
