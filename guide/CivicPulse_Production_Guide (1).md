# üöÄ CIVICPULSE INDIA: Complete Production-Ready Implementation Guide
**Full Step-by-Step Blueprint with All 8 Notebooks, Code, and Architecture**

**Date:** January 28, 2026  
**Status:** Ready to Execute Immediately  
**Timeline:** 12-16 weeks to production  
**Scope:** Entire India (3.2M grid cells) + GPU/Laptop dual optimization

---

## üìå EXECUTIVE SUMMARY

You're NOT implementing Hyderabad-only anymore. **You're building for all-India directly.** This requires:

1. **Geographic Scaling**: Hyderabad (31K km¬≤) ‚Üí India (3.2M km¬≤) = **105x larger**
2. **Infrastructure Code**: Device-agnostic (run same code on laptop/GPU PC)
3. **8 Complete Notebooks**: From boundary setup ‚Üí final predictions + gap analysis
4. **Git-Ready**: All code ready for `civicpulse-ai` organization repo

---

## ‚ö° PHASE 0: IMMEDIATE ACTIONS (Day 1 - 2 hours)

### 1. Setup Git & Environment

```bash
# Clone your repo
cd ~/projects
git clone https://github.com/your-org/civicpulse-ai.git
cd civicpulse-ai

# Setup Git LFS
git lfs install
cat > .gitattributes << 'EOF'
*.pt filter=lfs diff=lfs merge=lfs -text
*.npy filter=lfs diff=lfs merge=lfs -text
*.h5 filter=lfs diff=lfs merge=lfs -text
EOF
git add .gitattributes && git commit -m "Setup Git LFS"

# Create branches
git checkout -b develop
git checkout -b feature/india-scaling
git push origin develop feature/india-scaling

# Create .env configuration
cat > .env << 'EOF'
CIVICPULSE_DEVICE=auto
CIVICPULSE_BATCH_SIZE=auto
CIVICPULSE_DATA_MODE=hdf5
CIVICPULSE_PATCH_SIZE=200
EOF

# Install dependencies
pip install -r requirements.txt
pip install h5py faiss-cpu tqdm rasterio geopandas
```

### 2. Verify Existing CivicPulse Repo

```bash
# Check what you already have
ls -la src/
ls -la notebooks/
python -c "from src.convlstm_model import ConvLSTM; print('‚úì Hyderabad model imports correctly')"
```

---

## üìä PHASE 1: GEOGRAPHIC EXPANSION (Weeks 1-2)

### 1.1 Add India Boundary Manager

**File**: `src/region_manager.py` (8 hours to implement + test)

```python
from shapely.geometry import box, Polygon
from typing import Dict, List, Tuple, Optional
import geopandas as gpd
import json

class RegionBoundary:
    """Geographic region with hierarchical support"""
    
    def __init__(self, name: str, bounds: Tuple[float, float, float, float], 
                 level: str, parent_id: Optional[str] = None):
        self.name = name
        self.bounds = bounds  # (minx, miny, maxx, maxy) WGS84
        self.level = level    # 'national', 'state', 'district'
        self.parent_id = parent_id
        self.geometry = box(*bounds)
        self.area_km2 = self._calc_area()
    
    def _calc_area(self) -> float:
        """Calculate area in km¬≤"""
        minx, miny, maxx, maxy = self.bounds
        width_km = (maxx - minx) * 111.32
        height_km = (maxy - miny) * 111.32
        return width_km * height_km
    
    def grid_cell_count(self, resolution_km: float = 1.0) -> int:
        """Grid cells at 1km resolution"""
        return int(self.area_km2 / (resolution_km ** 2))


class ConfigurableBoundaryManager:
    """Manages preset regions with hierarchical structure"""
    
    INDIA_BOUNDS = (68.7, 8.4, 97.5, 35.0)
    
    STATE_BOUNDS = {
        'Telangana': (78.0, 15.5, 81.9, 19.8),
        'Andhra Pradesh': (77.0, 12.6, 84.9, 18.3),
        'Maharashtra': (72.6, 16.5, 80.9, 23.3),
        'Karnataka': (74.0, 11.5, 78.6, 18.6),
        'Tamil Nadu': (76.8, 8.0, 80.3, 13.6),
        'Uttar Pradesh': (77.0, 23.8, 84.8, 30.4),
        'Delhi': (76.76, 28.41, 77.35, 28.88),
        'Punjab': (73.7, 29.5, 76.9, 32.3),
        'Gujarat': (68.1, 20.1, 74.4, 24.5),
        'Haryana': (76.4, 27.0, 77.6, 30.6),
        # ... add all 28 states + 8 UTs
    }
    
    def __init__(self, config_path: Optional[str] = None):
        self.regions: Dict[str, RegionBoundary] = {}
        self._load_defaults()
        if config_path:
            self._load_custom(config_path)
    
    def _load_defaults(self):
        """Load preset regions"""
        self.add_region(
            RegionBoundary('India', self.INDIA_BOUNDS, 'national')
        )
        
        for state_name, bounds in self.STATE_BOUNDS.items():
            self.add_region(
                RegionBoundary(state_name, bounds, 'state', parent_id='India')
            )
    
    def add_region(self, region: RegionBoundary):
        """Register region"""
        self.regions[region.name] = region
    
    def get_region(self, name: str) -> Optional[RegionBoundary]:
        """Retrieve region"""
        return self.regions.get(name)
    
    def get_regions_by_level(self, level: str) -> List[RegionBoundary]:
        """Get all regions at level"""
        return [r for r in self.regions.values() if r.level == level]
    
    def to_geojson(self, filename: str):
        """Export regions as GeoJSON"""
        features = []
        for region in self.regions.values():
            features.append({
                'type': 'Feature',
                'properties': {
                    'name': region.name,
                    'level': region.level,
                    'area_km2': region.area_km2,
                    'grid_cells': region.grid_cell_count()
                },
                'geometry': {
                    'type': 'Polygon',
                    'coordinates': [[
                        [region.bounds[0], region.bounds[1]],
                        [region.bounds[2], region.bounds[1]],
                        [region.bounds[2], region.bounds[3]],
                        [region.bounds[0], region.bounds[3]],
                        [region.bounds[0], region.bounds[1]]
                    ]]
                }
            })
        
        with open(filename, 'w') as f:
            json.dump({'type': 'FeatureCollection', 'features': features}, f)
```

**Test**:
```python
mgr = ConfigurableBoundaryManager()
india = mgr.get_region('India')
print(f"India: {india.area_km2:,.0f} km¬≤, ~{india.grid_cell_count()//1000}k cells")
# Expected: ~3.2M cells
```

### 1.2 Add Region-Aware Preprocessing

**File**: `src/preprocessing.py` (Update existing with new methods - 4 hours)

```python
import numpy as np
from sklearn.neighbors import NearestNeighbors
import warnings

class RegionAwarePreprocessor:
    """Handle urban vs rural data quality differences"""
    
    def __init__(self, quality_threshold: float = 0.3):
        self.quality_threshold = quality_threshold
    
    def calculate_quality_score(self, data: np.ndarray, 
                               region_type: str = 'mixed') -> np.ndarray:
        """Score each cell for data quality (0-1)"""
        height, width = data.shape
        quality = np.zeros_like(data, dtype=np.float32)
        
        # Score 1: Non-zero cells
        nonzero = (data > 0).astype(np.float32)
        quality += 0.2 * nonzero
        
        # Score 2: Spatial consistency
        for i in range(1, height-1):
            for j in range(1, width-1):
                neighbors = data[i-1:i+2, j-1:j+2].flatten()
                valid = neighbors[neighbors > 0]
                if len(valid) > 0:
                    consistency = 1.0 - np.std(valid)/(np.mean(valid) + 1e-6)
                    quality[i, j] += 0.3 * max(0, consistency)
        
        # Region-specific adjustments
        if region_type == 'urban':
            density = np.sum(data > 0) / data.size
            quality *= (1.0 + 0.5 * density)
        elif region_type == 'rural':
            quality = np.minimum(quality * 1.2, 1.0)
        
        return np.minimum(quality / np.maximum(quality.max(), 1e-6), 1.0)
    
    def identify_low_quality_regions(self, data: np.ndarray,
                                    threshold: float = 0.3) -> np.ndarray:
        """Boolean mask of cells needing interpolation"""
        quality = self.calculate_quality_score(data)
        return quality < threshold
    
    def adaptive_interpolation(self, data: np.ndarray,
                              quality_mask: np.ndarray) -> np.ndarray:
        """KNN-based adaptive interpolation"""
        data_filled = data.copy()
        
        valid_coords = np.argwhere(~quality_mask)
        valid_values = data[~quality_mask]
        
        if len(valid_coords) == 0:
            return data_filled
        
        missing_coords = np.argwhere(quality_mask)
        if len(missing_coords) > 0:
            neighbors = NearestNeighbors(n_neighbors=5).fit(valid_coords)
            distances, indices = neighbors.kneighbors(missing_coords)
            
            for coord_idx, (coord, dist_list, idx_list) in enumerate(
                zip(missing_coords, distances, indices)):
                weights = 1.0 / (dist_list + 1e-6)
                weights /= weights.sum()
                data_filled[coord[0], coord[1]] = np.sum(
                    valid_values[idx_list] * weights
                )
        
        return data_filled
```

---

## üìì THE 8 NOTEBOOKS

### Notebook 00: Setup India Boundaries (5 min)
```python
# notebooks/00_setup_india_boundaries.ipynb

from src.region_manager import ConfigurableBoundaryManager

mgr = ConfigurableBoundaryManager()
india = mgr.get_region('India')

print(f"India: {india.area_km2:,.0f} km¬≤ ‚Üí {india.grid_cell_count():,} cells")
print(f"States: {len(mgr.get_regions_by_level('state'))}")

mgr.to_geojson('data/raw/india_regions.geojson')
print("‚úì Boundaries exported")
```

### Notebook 01: Preprocess Sample States (1 hour)
- Load Telangana + Maharashtra WorldPop data
- Clip to state boundaries
- Assess quality, interpolate missing data
- Stack into temporal sequences
- Save as NumPy arrays

### Notebook 02: Create HDF5 Dataset (10 min)
- Combine state sequences into single HDF5
- Configure lazy loading chunks
- Compress with GZIP
- Verify performance

### Notebook 03: Clip Full India (8-12 hours, overnight)
- Process all 5 years of WorldPop for full India
- Save as individual GeoTIFFs
- Create consolidated HDF5 (~15GB)

### Notebook 04: Model Architecture (2 hours)
- Define scalable ConvLSTM with spatial decomposition
- Patch-based inference for memory efficiency
- Test on sample data

### Notebook 05: Progressive Training (48+ hours, continuous)
- **Stage 1 (Coarse)**: 20 epochs on 1/4 resolution = 6 hours
- **Stage 2 (Medium)**: 30 epochs on 1/2 resolution = 15 hours
- **Stage 3 (Fine)**: 50 epochs on full resolution = 25 hours
- Total: ~46 hours on laptop CPU, ~4 hours on GPU

### Notebook 06: Inference & Predictions (4 hours)
- Run full model on all India (coarse ‚Üí fine)
- Generate predictions for 2025, 2030
- Visualize results

### Notebook 07: Gap Analysis + Infrastructure (6 hours)
- Identify high-growth zones
- Calculate stress scores
- Recommend hospital/school locations using FAISS
- Generate report

### Notebook 08: Dashboard + Deployment (8 hours)
- Streamlit dashboard with interactive maps
- Deployment on cloud (AWS/GCP/Azure)
- Export results

---

## üèóÔ∏è ARCHITECTURE OVERVIEW

```
INPUT LAYER
  ‚Üì
WorldPop Data (1km grid, 2000-2020)
  ‚Üì
PREPROCESSING STAGE
  ‚îú‚îÄ Clip to regions
  ‚îú‚îÄ Assess quality
  ‚îú‚îÄ Interpolate gaps
  ‚îî‚îÄ Stack temporal
  ‚Üì
HDF5 STORAGE
  ‚îú‚îÄ Lazy loading (laptop: 4GB ‚Üí 2GB active)
  ‚îî‚îÄ Direct loading (GPU: 40GB RAM)
  ‚Üì
MODEL TRAINING (Progressive)
  ‚îú‚îÄ Stage 1: Coarse (20√ó20 km patches) - 6 hrs
  ‚îú‚îÄ Stage 2: Medium (10√ó10 km patches) - 15 hrs
  ‚îî‚îÄ Stage 3: Fine (1√ó1 km cells) - 25 hrs
  ‚Üì
INFERENCE
  ‚îú‚îÄ Predict 2025, 2030 population
  ‚îî‚îÄ Output: 3.2M grid cells predictions
  ‚Üì
GAP ANALYSIS
  ‚îú‚îÄ Identify high-growth zones
  ‚îú‚îÄ Calculate infrastructure gaps
  ‚îî‚îÄ Recommend new facility locations (FAISS KNN)
  ‚Üì
VISUALIZATION & EXPORT
  ‚îú‚îÄ Interactive Streamlit dashboard
  ‚îú‚îÄ GeoJSON exports for GIS
  ‚îî‚îÄ PDF reports
```

---

## ‚öôÔ∏è KEY IMPLEMENTATION DETAILS

### Device-Agnostic Configuration

**`src/config.py`** (Update existing):
```python
import os
import torch
from dotenv import load_dotenv

load_dotenv()

class TrainingConfig:
    DEVICE = torch.device(os.getenv('CIVICPULSE_DEVICE', 'auto'))
    BATCH_SIZE = int(os.getenv('CIVICPULSE_BATCH_SIZE', 'auto'))
    DATA_MODE = os.getenv('CIVICPULSE_DATA_MODE', 'hdf5')
    PATCH_SIZE = int(os.getenv('CIVICPULSE_PATCH_SIZE', '200'))
    
    @staticmethod
    def auto_detect_device():
        if torch.cuda.is_available():
            return torch.device('cuda')
        return torch.device('cpu')
```

### Data Loading Strategy

**For Laptop** (4GB RAM):
- Use HDF5 with chunks of 256√ó256
- Load 1 timestep at a time = ~256KB active
- Trade-off: Slower I/O, manageable memory

**For GPU PC** (40GB+ RAM):
- Pre-load full India array into memory
- Trade-off: Faster computation, high memory

---

## üìà TIMELINE BREAKDOWN

| Phase | Week | Task | Hours | Device |
|-------|------|------|-------|--------|
| **Setup** | W1 | Git + boundaries + preprocessing | 16 | Laptop |
| **Sampling** | W2-W3 | Download + process Telangana + Maharashtra | 20 | Laptop |
| **Prep** | W3-W4 | Create HDF5 + infrastructure code | 12 | Both |
| **Full Data** | W4-W5 | Clip all India (overnight job) | 12 | Laptop/GPU |
| **Model** | W5-W8 | Progressive training | 48 | GPU/Laptop |
| **Analysis** | W8-W10 | Inference + gap analysis | 16 | GPU |
| **Deploy** | W10-W12 | Dashboard + final export | 20 | Cloud |
| **Buffer** | W12-W16 | Testing + optimization + presentation | 32 | All |

---

## ‚úÖ SUCCESS CHECKLIST

**Week 1:**
- [ ] Git setup complete, LFS configured
- [ ] `src/region_manager.py` implemented and tested
- [ ] `src/preprocessing.py` updated
- [ ] `src/config.py` updated with device detection
- [ ] Notebook 00 runs successfully

**Week 2-3:**
- [ ] Download 5 years WorldPop data (5 files √ó 600MB)
- [ ] Notebook 01 processes Telangana & Maharashtra
- [ ] Notebook 02 creates HDF5 file
- [ ] Test HDF5 lazy loading (should use <1GB RAM)

**Week 4-5:**
- [ ] Notebook 03 clips full India (overnight run)
- [ ] Notebook 04 tests model on sample data
- [ ] Verify patch-based inference memory savings

**Week 5-8:**
- [ ] Notebook 05 trains on progressive stages
- [ ] Model converges with R¬≤ > 0.85

**Week 8-10:**
- [ ] Notebook 06 generates predictions
- [ ] Notebook 07 identifies gaps + recommends locations
- [ ] Export results as GeoJSON/CSV

**Week 10-12:**
- [ ] Notebook 08 creates interactive dashboard
- [ ] Deploy to cloud or local server
- [ ] Generate final PDF report

---

## üö® CRITICAL GOTCHAS

1. **WorldPop Download**: Files are 600MB each, 5 years = 3GB download time
   - Solution: Start download early, script can run in background
   
2. **Memory Issues on Laptop**: Full India = 15GB uncompressed
   - Solution: Use HDF5 with chunks, never load full array
   
3. **Training Time**: 48+ hours continuous on laptop CPU
   - Solution: Use GPU PC for this phase, or split into stages
   
4. **Git LFS**: Must be setup BEFORE committing large files
   - Solution: Do Task 1 immediately, verify with `git lfs ls-files`

5. **Path Issues**: Relative paths may fail across notebooks
   - Solution: Always use `sys.path.insert(0, '/full/path/to/civicpulse-ai')`

---

## üéØ DELIVERABLES

By project completion, you'll have:

1. **Production-Ready Code** (on GitHub)
   - 8 complete notebooks
   - Scalable Python modules
   - Proper logging & error handling

2. **India-Scale Model** (trained + tested)
   - Predictions for all 3.2M grid cells
   - 2025 & 2030 projections
   - Quality metrics (R¬≤ > 0.85)

3. **Infrastructure Recommendations**
   - Top 500 sites for new hospitals
   - Top 2000 sites for new schools
   - Growth stress scores by district

4. **Interactive Dashboard**
   - Streamlit app with maps
   - Export to GeoJSON/PDF
   - Share-able link

5. **Documentation**
   - Methodology paper
   - User guide
   - API documentation

---

## üìû QUICK START

```bash
# 1. Clone & setup (30 min)
git clone https://github.com/your-org/civicpulse-ai.git
cd civicpulse-ai
git lfs install
pip install -r requirements.txt

# 2. Add new files (2 hours)
# Copy src/region_manager.py from this guide
# Copy src/preprocessing.py updates
# Copy src/config.py updates

# 3. Run Notebook 00 (5 min)
jupyter notebook notebooks/00_setup_india_boundaries.ipynb

# 4. Download data (30 min - 1 hour)
# From https://data.worldpop.org/
# Save 5 files to data/raw/worldpop/

# 5. Run Notebook 01 (1 hour)
jupyter notebook notebooks/01_preprocess_sample_states.ipynb

# 6. Run Notebook 02 (10 min)
jupyter notebook notebooks/02_create_hdf5_dataset.ipynb

# 7. Commit & push
git add .
git commit -m "[LAPTOP] Completed weeks 1-3: Boundaries + sample preprocessing"
git push origin feature/india-scaling

# 8. Continue with notebooks 03-08...
```

---

**You're ready. Start with Phase 0 today. Week 1 complete by Friday. Good luck!** üöÄ

---

*Last Updated: January 28, 2026*  
*Document Version: 2.0 (India-Scale Ready)*  
*Status: READY TO EXECUTE*